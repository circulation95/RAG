import streamlit as st
from langchain_core.messages.chat import ChatMessage
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_core.output_parsers import StrOutputParser
from langchain_teddynote import logging

from dotenv import load_dotenv
import os

# .env ë¡œë“œ
load_dotenv()
logging.langsmith("[Project] Multi Turn ì±—ë´‡")

# ìºì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
for folder in [".cache", ".cache/files", ".cache/embeddings"]:
    os.makedirs(folder, exist_ok=True)

# Streamlit ì œëª©
st.title("ëŒ€í™”ë‚´ìš©ì„ ê¸°ì–µí•˜ëŠ” ì±—ë´‡ ğŸ’¬")

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state["messages"] = []

if "store" not in st.session_state:
    st.session_state["store"] = {}

if "chain" not in st.session_state:
    st.session_state["chain"] = None

# ì‚¬ì´ë“œë°”
with st.sidebar:
    clear_btn = st.button("ëŒ€í™” ì´ˆê¸°í™”")
    selected_model = st.selectbox("LLM ì„ íƒ", ["gpt-4.1-mini", "gpt-4.1-nano"], index=0)
    session_id = st.text_input("ì„¸ì…˜ IDë¥¼ ì…ë ¥í•˜ì„¸ìš”.", "abc123")


# ëŒ€í™” ê¸°ë¡ ì¶œë ¥
def print_messages():
    for msg in st.session_state["messages"]:
        st.chat_message(msg.role).write(msg.content)


# ë©”ì‹œì§€ ì¶”ê°€
def add_message(role, content):
    st.session_state["messages"].append(ChatMessage(role=role, content=content))


# ì„¸ì…˜ë³„ ëŒ€í™”ê¸°ë¡ ê´€ë¦¬
def get_session_history(session_ids):
    if session_ids not in st.session_state["store"]:
        st.session_state["store"][session_ids] = ChatMessageHistory()
    return st.session_state["store"][session_ids]


# ì²´ì¸ ìƒì„±
def create_chain(model_name="gpt-4.1-mini"):
    prompt = ChatPromptTemplate.from_messages(
        [
            (
                "system",
                "ë‹¹ì‹ ì€ Question-Answering ì±—ë´‡ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.",
            ),
            MessagesPlaceholder(variable_name="chat_history"),
            ("human", "#Question:\n{question}"),
        ]
    )

    llm = ChatOpenAI(model_name=model_name)
    chain = prompt | llm | StrOutputParser()

    chain_with_history = RunnableWithMessageHistory(
        chain,
        get_session_history,
        input_messages_key="question",
        history_messages_key="chat_history",
    )
    return chain_with_history


# ì´ˆê¸°í™” ë²„íŠ¼
if clear_btn:
    st.session_state["messages"] = []
    st.session_state["chain"] = create_chain(model_name=selected_model)

# ì´ì „ ëŒ€í™” ì¶œë ¥
print_messages()

# ì²´ì¸ì´ ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±
if st.session_state["chain"] is None:
    st.session_state["chain"] = create_chain(model_name=selected_model)

# ì‚¬ìš©ì ì…ë ¥
USER_INPUT = st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”.")
warning_msg = st.empty()

# ì…ë ¥ì´ ë“¤ì–´ì˜¤ë©´
if USER_INPUT:
    chain = st.session_state["chain"]
    if chain is not None:
        response = chain.stream(
            {"question": USER_INPUT},
            config={"configurable": {"session_id": session_id}},
        )

        st.chat_message("user").write(USER_INPUT)

        with st.chat_message("assistant"):
            container = st.empty()
            ai_answer = ""

            for token in response:
                ai_answer += token
                container.markdown(ai_answer)

            add_message("user", USER_INPUT)
            add_message("assistant", ai_answer)
    else:
        warning_msg.error("Chainì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
