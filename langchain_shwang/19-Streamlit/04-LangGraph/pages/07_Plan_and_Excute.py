import streamlit as st
import operator
from typing import List, Union, Tuple, Annotated, TypedDict, Optional
from dotenv import load_dotenv
from langchain_teddynote import logging

# LangChain & LangGraph Imports
from langchain_openai import ChatOpenAI
from langchain_community.tools.tavily_search import TavilySearchResults
from langchain_core.prompts import ChatPromptTemplate
from pydantic import BaseModel, Field
from langchain_core.output_parsers import StrOutputParser
from langgraph.graph import END, StateGraph, START
from langgraph.checkpoint.memory import MemorySaver
from langgraph.prebuilt import create_react_agent
from langchain_core.tools import create_retriever_tool
from langchain_community.document_loaders import PyPDFLoader
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ (.env íŒŒì¼ì— OPENAI_API_KEY, TAVILY_API_KEY í•„ìš”)
load_dotenv()

logging.langsmith("PLAN & EXCUTE")

st.set_page_config(page_title="Plan and Execute Agent", layout="wide")
st.title("ğŸ¤– Plan and Execute Agent with PDF")

# --- 1. ìƒíƒœ ë° ëª¨ë¸ ì •ì˜ ---

class PlanExecute(TypedDict):
    input: Annotated[str, "User's input"]
    plan: Annotated[List[str], "Current plan"]
    past_steps: Annotated[List[Tuple], operator.add]
    response: Annotated[str, "Final response"]

class Plan(BaseModel):
    """Sorted steps to execute the plan"""
    steps: Annotated[List[str], "Different steps to follow, should be in sorted order"]

class Response(BaseModel):
    """Response to user."""
    response: str

class Act(BaseModel):
    """Action to perform."""
    
    # Union ëŒ€ì‹  Optional í•„ë“œ ë‘ ê°œë¡œ í‰íƒ„í™”(Flatten)í•©ë‹ˆë‹¤.
    response: Optional[str] = Field(
        description="Final response to the user. Use this when you have the answer.", 
        default=None
    )
    plan: Optional[List[str]] = Field(
        description="List of remaining steps to follow. Use this if you need to do more steps.", 
        default=None
    )

# --- 2. í—¬í¼ í•¨ìˆ˜ (PDF ì²˜ë¦¬ ë° ë„êµ¬ ì„¤ì •) ---

@st.cache_resource
def get_pdf_retriever(file):
    # ì„ì‹œ íŒŒì¼ ì €ì¥ ë° ë¡œë“œ
    file_path = f"./temp_{file.name}"
    with open(file_path, "wb") as f:
        f.write(file.read())
    
    loader = PyPDFLoader(file_path)
    docs = loader.load()
    
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    splits = text_splitter.split_documents(docs)
    
    vectorstore = FAISS.from_documents(documents=splits, embedding=OpenAIEmbeddings())
    retriever = vectorstore.as_retriever()
    return retriever

# --- 3. ë…¸ë“œ í•¨ìˆ˜ ì •ì˜ ---

def plan_step(state: PlanExecute):
    """1. ì´ˆê¸° ê³„íš ìˆ˜ë¦½ ë‹¨ê³„"""
    print("---PLAN STEP---")
    planner_prompt = ChatPromptTemplate.from_messages(
        [
            (
                "system",
                """For the given objective, come up with a simple step by step plan. \
This plan should involve individual tasks, that if executed correctly will yield the correct answer. Do not add any superfluous steps. \
The result of the final step should be the final answer. Make sure that each step has all the information needed - do not skip steps.
Answer in Korean.""",
            ),
            ("placeholder", "{messages}"),
        ]
    )
    
    # ëª¨ë¸ì€ st.session_stateì—ì„œ ê°€ì ¸ì˜´
    model = st.session_state["llm_model"]
    planner = planner_prompt | model.with_structured_output(Plan)
    plan = planner.invoke({"messages": [("user", state["input"])]})
    
    return {"plan": plan.steps}

def execute_step(state: PlanExecute):
    """2. ê³„íš ì‹¤í–‰ ë‹¨ê³„ (LangGraph Prebuilt Agent ì‚¬ìš©)"""
    print("---EXECUTE STEP---")
    plan = state["plan"]
    plan_str = "\n".join(f"{i+1}. {step}" for i, step in enumerate(plan))
    task = plan[0]
    
    # ì—ì´ì „íŠ¸ì—ê²Œ ì „ë‹¬í•  êµ¬ì²´ì ì¸ ì§€ì‹œì‚¬í•­
    task_formatted = f"""For the following plan:
{plan_str}

You are tasked with executing [step 1. {task}]."""

    # í˜„ì¬ ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ ë° ëª¨ë¸ ê°€ì ¸ì˜¤ê¸°
    current_tools = st.session_state["tools"]
    model = st.session_state["llm_model"]

    # 1. LangGraphìš© ReAct ì—ì´ì „íŠ¸ ìƒì„±
    # (prompt ëŒ€ì‹  state_modifierì— ì‹œìŠ¤í…œ ë©”ì‹œì§€ë¥¼ ë„£ìŠµë‹ˆë‹¤)
    agent_app = create_react_agent(
        model, 
        current_tools, 
        prompt="You are a helpful assistant. Answer in Korean."
    )
    
    # 2. Agent ì‹¤í–‰
    try:
        # LangGraph ì—ì´ì „íŠ¸ëŠ” ì…ë ¥ìœ¼ë¡œ {"messages": [...]}ë¥¼ ë°›ìŠµë‹ˆë‹¤.
        result = agent_app.invoke({"messages": [("human", task_formatted)]})
        
        # 3. ê²°ê³¼ íŒŒì‹± (ë§ˆì§€ë§‰ ë©”ì‹œì§€ê°€ AIì˜ ìµœì¢… ë‹µë³€ì…ë‹ˆë‹¤)
        result_content = result["messages"][-1].content
        
    except Exception as e:
        result_content = f"Error executing step: {str(e)}"

    return {
        "past_steps": [(task, result_content)],
    }

def replan_step(state: PlanExecute):
    """3. ì¬ê³„íš ë° ì¢…ë£Œ íŒë‹¨ ë‹¨ê³„"""
    print("---REPLAN STEP---")
    replanner_prompt = ChatPromptTemplate.from_template(
        """For the given objective, come up with a simple step by step plan. \
This plan should involve individual tasks, that if executed correctly will yield the correct answer. Do not add any superfluous steps. \
The result of the final step should be the final answer. Make sure that each step has all the information needed - do not skip steps.

Your objective was this:
{input}

Your original plan was this:
{plan}

You have currently done the follow steps:
{past_steps}

Update your plan accordingly. If no more steps are needed and you can return to the user, then respond with that. Otherwise, fill out the plan. Only add steps to the plan that still NEED to be done. Do not return previously done steps as part of the plan.

Answer in Korean."""
    )
    
    model = st.session_state["llm_model"]
    replanner = replanner_prompt | model.with_structured_output(Act)
    
    output = replanner.invoke(state)
    # 1. response í•„ë“œê°€ ì±„ì›Œì ¸ ìˆë‹¤ë©´ -> ì¢…ë£Œ ë° ë‹µë³€ ë°˜í™˜
    if output.response:
        return {"response": output.response}
    
    # 2. plan í•„ë“œê°€ ì±„ì›Œì ¸ ìˆë‹¤ë©´ -> ë‹¤ìŒ ê³„íš ì‹¤í–‰
    elif output.plan:
        return {"plan": output.plan}
        
    # 3. ì˜ˆì™¸ ì²˜ë¦¬ (ë‘˜ ë‹¤ ë¹„ì–´ìˆì„ ê²½ìš°)
    else:
        return {"response": "ì£„ì†¡í•©ë‹ˆë‹¤. ê³„íšì„ ìˆ˜ë¦½í•˜ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."}

def should_end(state: PlanExecute):
    """ì¡°ê±´ë¶€ ì—£ì§€ ë¡œì§"""
    if "response" in state and state["response"]:
        return "final_report"
    else:
        return "execute"

def generate_final_report(state: PlanExecute):
    """4. ìµœì¢… ë³´ê³ ì„œ ì‘ì„±"""
    print("---FINAL REPORT---")
    final_report_prompt = ChatPromptTemplate.from_template(
        """You are given the objective and the previously done steps. Your task is to generate a final report in markdown format.
Final report should be written in professional tone.

Your objective was this:
{input}

Your previously done steps(question and answer pairs):
{past_steps}

Generate a final report in markdown format. Write your response in Korean."""
    )
    
    model = st.session_state["llm_model"]
    final_report = final_report_prompt | model | StrOutputParser()
    
    # past_steps í¬ë§·íŒ…
    past_steps_str = "\n\n".join(
        [f"Question: {step[0]}\nAnswer: {step[1]}" for step in state["past_steps"]]
    )
    
    response = final_report.invoke({"input": state["input"], "past_steps": past_steps_str})
    return {"response": response}

# --- 4. ê·¸ë˜í”„ ë¹Œë“œ í•¨ìˆ˜ ---

def build_graph():
    workflow = StateGraph(PlanExecute)

    workflow.add_node("planner", plan_step)
    workflow.add_node("execute", execute_step)
    workflow.add_node("replan", replan_step)
    workflow.add_node("final_report", generate_final_report)

    workflow.add_edge(START, "planner")
    workflow.add_edge("planner", "execute")
    workflow.add_edge("execute", "replan")
    
    workflow.add_conditional_edges(
        "replan",
        should_end,
        {"execute": "execute", "final_report": "final_report"},
    )
    workflow.add_edge("final_report", END)

    return workflow.compile(checkpointer=MemorySaver())

# --- 5. Streamlit ì‚¬ì´ë“œë°” ë° ì´ˆê¸°í™” ---

with st.sidebar:
    st.header("ì„¤ì •")
    uploaded_file = st.file_uploader("PDF íŒŒì¼ì„ ì—…ë¡œë“œ (ì„ íƒ)", type=["pdf"])
    
    model_name = st.selectbox(
        "OpenAI ëª¨ë¸ ì„ íƒ",
        ["gpt-4o-mini", "gpt-4-turbo", "gpt-3.5-turbo"],
        index=0
    )
    
    if st.button("ëŒ€í™” ì´ˆê¸°í™”"):
        st.session_state["messages"] = []
        st.rerun()

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state["messages"] = []

# ëª¨ë¸ ë° ë„êµ¬ ì„¤ì • (ë§¤ë²ˆ ê°±ì‹ )
st.session_state["llm_model"] = ChatOpenAI(model=model_name, temperature=0)

# ê¸°ë³¸ ë„êµ¬: ì›¹ ê²€ìƒ‰
tools = [TavilySearchResults(max_results=3)]

# PDFê°€ ìˆìœ¼ë©´ ë„êµ¬ì— ì¶”ê°€
if uploaded_file:
    retriever = get_pdf_retriever(uploaded_file)
    retriever_tool = create_retriever_tool(
        retriever,
        "pdf_search",
        "Search for information about the uploaded PDF file."
    )
    tools.append(retriever_tool)
    st.success("PDF ì²˜ë¦¬ ì™„ë£Œ! ë„êµ¬ì— ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤.")

st.session_state["tools"] = tools

# ê·¸ë˜í”„ ìƒì„± (í•œ ë²ˆë§Œ í˜¹ì€ íŒŒì¼ ë³€ê²½ ì‹œ)
if "graph" not in st.session_state or uploaded_file:
    st.session_state["graph"] = build_graph()

# --- 6. ì±„íŒ… ì¸í„°í˜ì´ìŠ¤ ---

# ì´ì „ ë©”ì‹œì§€ ì¶œë ¥
for msg in st.session_state["messages"]:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

# ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
if prompt := st.chat_input("ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?"):
    # ì‚¬ìš©ì ë©”ì‹œì§€ ì €ì¥ ë° ì¶œë ¥
    st.session_state["messages"].append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # ê·¸ë˜í”„ ì‹¤í–‰ ì„¤ì •
    config = {"configurable": {"thread_id": "1"}}
    
    # ìŠ¤íŠ¸ë¦¬ë° ì‹¤í–‰ (ì…ë ¥ í‚¤ëŠ” 'input' ì´ì–´ì•¼ í•¨)
    inputs = {"input": prompt}
    
    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        full_response = ""
        
        # ì´ë²¤íŠ¸ ë£¨í”„: ì‹¤ì œ ê·¸ë˜í”„ì˜ ë…¸ë“œ ì´ë¦„ì— ë§ì¶° ì²˜ë¦¬
        events = st.session_state["graph"].stream(inputs, config=config)
        
        for event in events:
            for node_name, values in event.items():
                
                # 1. ê³„íš ìˆ˜ë¦½ ë‹¨ê³„
                if node_name == "planner":
                    plan_text = "\n".join([f"- {step}" for step in values["plan"]])
                    with st.expander("ğŸ“… ì´ˆê¸° ê³„íš ìˆ˜ë¦½", expanded=True):
                        st.markdown(plan_text)
                        
                # 2. ì‹¤í–‰ ë‹¨ê³„
                elif node_name == "execute":
                    last_step = values["past_steps"][-1] # (Task, Result)
                    with st.expander(f"âš™ï¸ ì‹¤í–‰ ì¤‘: {last_step[0]}", expanded=False):
                        st.write(last_step[1])
                        
                # 3. ì¬ê³„íš ë‹¨ê³„
                elif node_name == "replan":
                    if "response" in values:
                        pass # ì¢…ë£Œ ì‹ í˜¸
                    else:
                        new_plan = values.get("plan", [])
                        if new_plan:
                            with st.expander("ğŸ”„ ê³„íš ìˆ˜ì •ë¨", expanded=False):
                                st.markdown("\n".join([f"- {s}" for s in new_plan]))

                # 4. ìµœì¢… ë³´ê³ ì„œ
                elif node_name == "final_report":
                    full_response = values["response"]
                    message_placeholder.markdown(full_response)

    # ìµœì¢… ì‘ë‹µ ì €ì¥
    st.session_state["messages"].append({"role": "assistant", "content": full_response})