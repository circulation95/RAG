from typing import List, Union
from langchain_experimental.agents.agent_toolkits import create_pandas_dataframe_agent
from langchain_experimental.tools import PythonAstREPLTool
from langchain_openai import ChatOpenAI
from langchain_teddynote import logging
from langchain_teddynote.messages import AgentStreamParser, AgentCallbacks
from dotenv import load_dotenv
from rag.pdf import PDFRetrievalChain
import streamlit as st
import pandas as pd
import matplotlib.pyplot as plt
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder

from typing import Annotated, Sequence, TypedDict
from langchain_core.messages import (
    BaseMessage,
    AIMessage,
    HumanMessage,
    SystemMessage,
    ToolMessage,
)
from langgraph.graph.message import add_messages
from typing import Literal
from langchain import hub
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import PromptTemplate
from pydantic import BaseModel, Field
from langgraph.prebuilt import tools_condition
from langchain_teddynote.models import get_model_name, LLMs
from langchain_core.tools.retriever import create_retriever_tool

from langgraph.graph import END, StateGraph, START
from langgraph.prebuilt import ToolNode
from langgraph.checkpoint.memory import MemorySaver
from langchain_teddynote.graphs import visualize_graph

from langchain_core.runnables import RunnableConfig
from langchain_teddynote.messages import stream_graph, invoke_graph, random_uuid
from langchain_core.documents import Document
from langchain_teddynote.tools.tavily import TavilySearch

load_dotenv()
logging.langsmith("Self RAG")

st.title("Self RAG")

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state["messages"] = []

if "graph" not in st.session_state:
    st.session_state["graph"] = None

# ìƒìˆ˜ ì •ì˜
class MessageRole:
    USER = "user"
    TOOL = "tool"
    ASSISTANT = "assistant"


class MessageType:
    TEXT = "text"
    FIGURE = "figure"
    CODE = "code"
    DATAFRAME = "dataframe"


# State ì •ì˜
class State(TypedDict):
    question: Annotated[str, "The question to answer"]
    generation: Annotated[str, "The generation from the LLM"]
    documents: Annotated[List[str], "The documents retrieved"]


# ë°ì´í„° ëª¨ë¸ ì •ì˜: ê²€ìƒ‰ëœ ë¬¸ì„œì˜ ê´€ë ¨ì„±ì„ ì´ì§„ ì ìˆ˜ë¡œ í‰ê°€í•˜ê¸° ìœ„í•œ ë°ì´í„° ëª¨ë¸
class GradeDocuments(BaseModel):
    """A binary score to determine the relevance of the retrieved documents."""

    # ë¬¸ì„œê°€ ì§ˆë¬¸ì— ê´€ë ¨ì´ ìˆëŠ”ì§€ ì—¬ë¶€ë¥¼ 'yes' ë˜ëŠ” 'no'ë¡œ ë‚˜íƒ€ë‚´ëŠ” í•„ë“œ
    binary_score: str = Field(
        description="Documents are relevant to the question, 'yes' or 'no'"
    )

# ë°ì´í„° ëª¨ë¸ ì •ì˜: ìƒì„±ëœ ë‹µë³€ì´ ì‚¬ì‹¤ì— ê¸°ë°˜í•˜ê³  ìˆëŠ”ì§€ ì—¬ë¶€ë¥¼ ì´ì§„ ì ìˆ˜ë¡œ í‰ê°€í•˜ê¸° ìœ„í•œ ë°ì´í„° ëª¨ë¸
class Groundednesss(BaseModel):
    """A binary score indicating whether the generated answer is grounded in the facts."""

    # ë‹µë³€ì´ ì‚¬ì‹¤ì— ê¸°ë°˜í•˜ê³  ìˆëŠ”ì§€ ì—¬ë¶€ë¥¼ 'yes' ë˜ëŠ” 'no'ë¡œ ë‚˜íƒ€ë‚´ëŠ” í•„ë“œ
    binary_score: str = Field(
        description="Answer is grounded in the facts, 'yes' or 'no'"
    )

class GradeAnswer(BaseModel):
    """A binary score indicating whether the question is addressed."""

    # ë‹µë³€ì˜ ê´€ë ¨ì„± í‰ê°€: 'yes' ë˜ëŠ” 'no'ë¡œ í‘œê¸°(yes: ê´€ë ¨ì„± ìˆìŒ, no: ê´€ë ¨ì„± ì—†ìŒ)
    binary_score: str = Field(
        description="Answer addresses the question, 'yes' or 'no'"
    )


# í•¨ìˆ˜ ì •ì˜ (ê¸°ì¡´ ìœ ì§€)
def format_docs(docs):
    return "\n\n".join(
        [
            f'<document><content>{doc.page_content}</content><source>{doc.metadata["source"]}</source><page>{doc.metadata["page"]+1}</page></document>'
            for doc in docs
        ]
    )


def embed_file(file):
    file_content = file.read()
    file_path = f".cache/files/{file.name}"
    with open(file_path, "wb") as f:
        f.write(file_content)

    pdf = PDFRetrievalChain([file_path]).create_chain()
    return pdf

# ë…¸ë“œ ì •ì˜
def retrieve(state: State):
    print("\n==== RETRIEVE ====\n")
    query = state["question"]
    # st.session_stateì— ì €ì¥ëœ retriever ì‚¬ìš©
    if "pdf_retriever" not in st.session_state:
        return {"documents": []}
        
    docs = st.session_state["pdf_retriever"].invoke(query)
    return {"documents": docs}

def generate(state: State):
    print("\n==== GENERATE ====\n")
    
    prompt = hub.pull("teddynote/rag-prompt")

    llm = ChatOpenAI(model = selected_model, temperature=0)

    rag_chain = prompt | llm | StrOutputParser()

    question = state["question"]
    documents = state["documents"]

    # RAGë¥¼ ì‚¬ìš©í•œ ë‹µë³€ ìƒì„±
    generation = rag_chain.invoke({"context": documents, "question": question})
    return {"generation": generation}
    

def grade_documents(state: State):
    print("\n==== [CHECK DOCUMENT RELEVANCE TO QUESTION] ====\n")
    question = state["question"]
    documents = state["documents"]
    
    llm = ChatOpenAI(model=selected_model, temperature=0)
    structured_llm_grader = llm.with_structured_output(GradeDocuments)

    # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì •ì˜
    system = """You are a grader assessing relevance of a retrieved document to a user question. \n 
    If the document contains keyword(s) or semantic meaning related to the question, grade it as relevant. \n
    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question."""

    grade_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", system),
            ("human", "Retrieved document: \n\n {document} \n\n User question: {question}") 
        ]
    )

    retrieval_grader = grade_prompt | structured_llm_grader

    filter_docs = []
    relevant_docs = 0

    for doc in documents:
        score = retrieval_grader.invoke(
            {"question": question, "document": doc.page_content}
        )
        grade = score.binary_score

        if grade == "yes":
            print("==== [GRADE: DOCUMENT RELEVANT] ====")
            filter_docs.append(doc)
            relevant_docs += 1
        else:
            print("==== [GRADE: DOCUMENT NOT RELEVANT] ====")
            continue
    return {"documents": filter_docs}

def query_rewrite(state: State):
    print("\n==== [REWRITE QUERY] ====\n")
    question = state["question"]

    llm = ChatOpenAI(model = selected_model, temperature=0)
        
    # Query Rewrite ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
    system = """You a question re-writer that converts an input question to a better version that is optimized 
    for web search. Look at the input and try to reason about the underlying semantic intent / meaning."""

    # í”„ë¡¬í”„íŠ¸ ì •ì˜
    re_write_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", system),
            (
                "human",
                "Here is the initial question: \n\n {question} \n Formulate an improved question.",
            ),
        ]
    )
    
    # Question Re-writer ì²´ì¸ ì´ˆê¸°í™”
    question_rewriter = re_write_prompt | llm | StrOutputParser()

    better_question = question_rewriter.invoke({"question":question})

    return {"question": better_question}

def decide_to_generation(state: State):
    # í‰ê°€ëœ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹¤ìŒ ë‹¨ê³„ ê²°ì •
    print("==== [ASSESS GRADED DOCUMENTS] ====")
    filtered_documents = state["documents"]

    if not filtered_documents:
        # ì›¹ ê²€ìƒ‰ìœ¼ë¡œ ì •ë³´ ë³´ê°•ì´ í•„ìš”í•œ ê²½ìš°
        print(
            "==== [DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, QUERY REWRITE] ===="
        )
        # ì¿¼ë¦¬ ì¬ì‘ì„± ë…¸ë“œë¡œ ë¼ìš°íŒ…
        return "query_rewrite"
    else:
        # ê´€ë ¨ ë¬¸ì„œê°€ ì¡´ì¬í•˜ë¯€ë¡œ ë‹µë³€ ìƒì„± ë‹¨ê³„(generate) ë¡œ ì§„í–‰
        print("==== [DECISION: GENERATE] ====")
        return "generate"


# ìƒì„±ëœ ë‹µë³€ì˜ ë¬¸ì„œ ë° ì§ˆë¬¸ê³¼ì˜ ê´€ë ¨ì„± í‰ê°€
def grade_generation_v_documents_and_question(state):
    print("==== [CHECK HALLUCINATIONS] ====")
    question = state["question"]
    documents = state["documents"]
    generation = state["generation"]

    llm = ChatOpenAI(model = selected_model, temperature=0)

    structured_llm_grader = llm.with_structured_output(Groundednesss)
    # Grounded check ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì •ì˜
    system = """You are a grader assessing whether an LLM generation is grounded in / supported by a set of retrieved facts. \n 
    Give a binary score 'yes' or 'no'. 'Yes' means that the answer is grounded in / supported by the set of facts."""
    
    # ì±„íŒ… í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìƒì„±
    groundedness_checking_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", system),
            ("human", "Set of facts: \n\n {documents} \n\n LLM generation: {generation}"),
        ]
    )
    groundedness_grader = groundedness_checking_prompt | structured_llm_grader

    
    # llm ì— GradeAnswer ë°”ì¸ë”©
    structured_llm_grader = llm.with_structured_output(GradeAnswer)

    # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì •ì˜
    system = """You are a grader assessing whether an answer addresses / resolves a question \n 
        Give a binary score 'yes' or 'no'. Yes' means that the answer resolves the question."""

    # í”„ë¡¬í”„íŠ¸ ìƒì„±
    answer_grader_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", system),
            ("human", "User question: \n\n {question} \n\n LLM generation: {generation}"),
        ]
    )

    # ë‹µë³€ í‰ê°€ê¸° ìƒì„±
    answer_grader = answer_grader_prompt | structured_llm_grader

    score = groundedness_grader.invoke(
        {"documents": documents, "generation": generation}
    )
    grade = score.binary_score

    # í™˜ê° ì—¬ë¶€ í™•ì¸
    if grade == "yes":
        print("==== [DECISION: GENERATION IS GROUNDED IN DOCUMENTS] ====")
        # ì§ˆë¬¸ í•´ê²° ì—¬ë¶€ í™•ì¸
        print("==== [GRADE GENERATION vs QUESTION] ====")
        score = answer_grader.invoke({"question": question, "generation": generation})
        grade = score.binary_score
        if grade == "yes":
            print("==== [DECISION: GENERATION ADDRESSES QUESTION] ====")
            return "relevant"
        else:
            print("==== [DECISION: GENERATION DOES NOT ADDRESS QUESTION] ====")
            return "not relevant"
    else:
        print("==== [DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY] ====")
        return "hallucination"

def build_graph():
    # ê·¸ë˜í”„ ìƒíƒœ ì´ˆê¸°í™”
    workflow = StateGraph(State)

    # [ì¤‘ìš”] ë…¸ë“œ ë“±ë¡ (ì´ë¦„ì„ ëª¨ë‘ í™•ì¸í•´ì£¼ì„¸ìš”!)
    workflow.add_node("retrieve", retrieve)
    workflow.add_node("grade_documents", grade_documents)
    workflow.add_node("generate", generate)
    # ì•„ë˜ ì¤„ì´ "transform_query"ê°€ ì•„ë‹ˆë¼ "query_rewrite"ì—¬ì•¼ í•©ë‹ˆë‹¤.
    workflow.add_node("query_rewrite", query_rewrite) 

    # ì—£ì§€ ì •ì˜
    workflow.add_edge(START, "retrieve")
    workflow.add_edge("retrieve", "grade_documents")

    # ë¬¸ì„œ í‰ê°€ ë…¸ë“œì—ì„œ ì¡°ê±´ë¶€ ì—£ì§€ ì¶”ê°€
    workflow.add_conditional_edges(
        "grade_documents",
        decide_to_generation,
        {
            "query_rewrite": "query_rewrite", # [í™•ì¸] í‚¤ì™€ ê°’ ëª¨ë‘ query_rewrite
            "generate": "generate",
        },
    )

    # [ì—ëŸ¬ ë°œìƒ ì§€ì  í•´ê²°] ì´ì œ "query_rewrite" ë…¸ë“œê°€ ë“±ë¡ë˜ì—ˆìœ¼ë¯€ë¡œ ì—ëŸ¬ê°€ ë‚˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
    workflow.add_edge("query_rewrite", "retrieve")

    # ë‹µë³€ ìƒì„± ë…¸ë“œì—ì„œ ì¡°ê±´ë¶€ ì—£ì§€ ì¶”ê°€
    workflow.add_conditional_edges(
        "generate",
        grade_generation_v_documents_and_question,
        {
            "hallucination": "generate",
            "relevant": END,
            "not relevant": "query_rewrite", # [í™•ì¸] ì—¬ê¸°ë„ query_rewrite
        },
    )

    return workflow.compile(checkpointer=MemorySaver())


def print_messages():
    for role, content_list in st.session_state["messages"]:
        with st.chat_message(role):
            for content in content_list:
                if isinstance(content, list):
                    if len(content) == 2:
                        message_type, message_content = content
                    elif len(content) == 1:
                        message_type = MessageType.TEXT
                        message_content = content[0]
                    else:
                        continue

                    if message_type == MessageType.TEXT:
                        st.markdown(message_content)
                    elif message_type == MessageType.FIGURE:
                        st.pyplot(message_content)
                    elif message_type == MessageType.CODE:
                        st.code(message_content, language="python")
                    elif message_type == MessageType.DATAFRAME:
                        st.dataframe(message_content)
                elif isinstance(content, str):
                    st.markdown(content)


def add_message(role: MessageRole, content: List[Union[MessageType, str]]):
    messages = st.session_state["messages"]
    if messages and messages[-1][0] == role:
        messages[-1][1].extend([content])
    else:
        messages.append([role, [content]])


# --- ì‚¬ì´ë“œë°” ì„¤ì • ---
with st.sidebar:
    
    # PDF ë¡œì§ì´ë¯€ë¡œ typeì„ pdfë¡œ ë³€ê²½
    uploaded_file = st.file_uploader("PDF íŒŒì¼ì„ ì—…ë¡œë“œ í•´ì£¼ì„¸ìš”.", type=["pdf"])

    selected_model = st.selectbox(
        "OpenAI ëª¨ë¸ì„ ì„ íƒí•´ì£¼ì„¸ìš”.",
        ["gpt-4o-mini", "gpt-4-turbo", "gpt-3.5-turbo"],
        index=0,
    )

    clear_btn = st.button("ëŒ€í™” ì´ˆê¸°í™”")

    if st.button("ê·¸ë˜í”„ êµ¬ì¡° ë³´ê¸°"):
        graph_img = st.session_state["graph"].get_graph().draw_mermaid_png()
        st.image(graph_img)


# ì§ˆë¬¸ ì²˜ë¦¬ í•¨ìˆ˜ (ìˆ˜ì •ë¨: name, instructions ì „ë‹¬)
# ì§ˆë¬¸ ì²˜ë¦¬ í•¨ìˆ˜ (ìˆ˜ì •ë¨: stream ì‚¬ìš©)
def ask(query):
    # 1. ì´ˆê¸° íŠ¸ë¦¬ê±° ë©”ì‹œì§€(ì‚¬ìš©ì ì…ë ¥) ì €ì¥ ë° ì¶œë ¥
    add_message(MessageRole.USER, [MessageType.TEXT, query])
    with st.chat_message("user"):
        st.write(query)

    graph = st.session_state["graph"]

    if "thread_id" not in st.session_state:
        st.session_state["thread_id"] = random_uuid()

    config = RunnableConfig(
        recursion_limit=20,  # ëŒ€í™”ê°€ ê¸¸ì–´ì§ˆ ìˆ˜ ìˆìœ¼ë‹ˆ ì œí•œì„ ì¡°ê¸ˆ ëŠ˜ë¦¼
        configurable={"thread_id": st.session_state["thread_id"]},
    )

    # 2. Graph ìŠ¤íŠ¸ë¦¬ë° ì‹¤í–‰
    # graph.streamì„ ì“°ë©´ ë…¸ë“œ í•˜ë‚˜ê°€ ëë‚  ë•Œë§ˆë‹¤ eventë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    events = graph.stream(
        {"question": query},
        config=config,
    )

# 3. ì´ë²¤íŠ¸ ë£¨í”„: ê° ë…¸ë“œ(AI, ì‹œë®¬ë ˆì´ì…˜ ìœ ì €)ì˜ ì¶œë ¥ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ì²˜ë¦¬for event in events:
    for event in events:
        for node_name, values in event.items():
            content = ""
            st_role = "assistant"
            
            # ë…¸ë“œë³„ë¡œ ì¶œë ¥í•  ë‚´ìš© ê²°ì •
            if node_name == "retrieve":
                content = f"ğŸ“„ ë¬¸ì„œë¥¼ {len(values['documents'])}ê°œ ê²€ìƒ‰í–ˆìŠµë‹ˆë‹¤."
                st_role = "tool"
            elif node_name == "grade_documents":
                # web_search ê°’ì— ë”°ë¼ ìƒíƒœ ì¶œë ¥
                need_search = values.get("web_search", "no")
                content = f"ğŸ” ë¬¸ì„œ í‰ê°€ ì™„ë£Œ. (ì›¹ ê²€ìƒ‰ í•„ìš”: {need_search})"
                st_role = "tool"
            elif node_name == "query_rewrite":
                new_q = values.get("question", "")
                content = f"ğŸ”„ ì§ˆë¬¸ì„ ì¬ì‘ì„±í–ˆìŠµë‹ˆë‹¤: {new_q}"
                st_role = "tool"
            elif node_name == "web_search_node":
                content = "ğŸŒ ì›¹ ê²€ìƒ‰ì„ ìˆ˜í–‰í•˜ì—¬ ì •ë³´ë¥¼ ë³´ê°•í–ˆìŠµë‹ˆë‹¤."
                st_role = "tool"
            elif node_name == "generate":
                # ìµœì¢… ë‹µë³€
                content = values["generation"]
                st_role = "assistant"
            
            # ë‚´ìš©ì´ ìˆìœ¼ë©´ ì¶œë ¥ ë° ì €ì¥
            if content:
                with st.chat_message(st_role):
                    st.markdown(content)
                add_message(st_role, [MessageType.TEXT, content])

# ë©”ì¸ ë¡œì§
if clear_btn:
    st.session_state["messages"] = []
    st.session_state.pop("thread_id", None)  # ìƒˆ threadë¡œ ì‹œì‘

if st.session_state["graph"] is None:
    st.session_state["graph"] = build_graph()

if uploaded_file:
    pdf = embed_file(uploaded_file)
    pdf_retriever = pdf.retriever
    pdf_chain = pdf.chain
    st.session_state["pdf_retriever"] = pdf.retriever
    st.session_state["graph"] = build_graph()
    st.success("ì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ! ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”.")

print_messages()
# ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬ (ì±„íŒ…ë°”)
user_input = st.chat_input("ê¶ê¸ˆí•œ ë‚´ìš©ì„ ë¬¼ì–´ë³´ì„¸ìš”!")

if user_input:
    ask(user_input)
