import streamlit as st
import operator
import os
from typing import List, Union, Tuple, Annotated, TypedDict, Optional, Sequence
from dotenv import load_dotenv
from langchain_teddynote import logging

# LangChain & LangGraph Imports
from langchain_openai import ChatOpenAI
from langchain_community.tools.tavily_search import TavilySearchResults
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.messages import BaseMessage, HumanMessage
from langchain_core.documents import Document
from pydantic import BaseModel, Field
from langchain_core.output_parsers import StrOutputParser
from langgraph.graph import END, StateGraph, START, MessagesState
from langgraph.checkpoint.memory import MemorySaver
from langgraph.prebuilt import create_react_agent
from langchain_core.tools import create_retriever_tool
from langchain_community.document_loaders import PyPDFLoader
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter

from langchain_teddynote.tools.tavily import TavilySearch
from langchain_core.tools import tool
from langchain_experimental.utilities import PythonREPL

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ (.env íŒŒì¼ì— OPENAI_API_KEY, TAVILY_API_KEY í•„ìš”)
load_dotenv()

logging.langsmith("Multi Agent")

st.set_page_config(page_title="Multi Agent", layout="wide")
st.title("ğŸ¤– Multi Agent with PDF")

# --- 1. ìƒíƒœ ë° ëª¨ë¸ ì •ì˜ ---

# ìƒíƒœ ì •ì˜
class AgentState(TypedDict):
    messages: Annotated[
        Sequence[BaseMessage], operator.add
    ]  # Agent ê°„ ê³µìœ í•˜ëŠ” ë©”ì‹œì§€ ëª©ë¡
    sender: Annotated[str, "The sender of the last message"]  # ë§ˆì§€ë§‰ ë©”ì‹œì§€ì˜ ë°œì‹ ì
    documents: Annotated[List[Document], "The documents retrieved"]

class GradeDocuments(BaseModel):
    """A binary score to determine the relevance of the retrieved documents."""

    # ë¬¸ì„œê°€ ì§ˆë¬¸ì— ê´€ë ¨ì´ ìˆëŠ”ì§€ ì—¬ë¶€ë¥¼ 'yes' ë˜ëŠ” 'no'ë¡œ ë‚˜íƒ€ë‚´ëŠ” í•„ë“œ
    binary_score: str = Field(
        description="Documents are relevant to the question, 'yes' or 'no'"
    )

# Tavily ê²€ìƒ‰ ë„êµ¬ ì •ì˜
tavily_tool = TavilySearch(max_results=5)

# Python ì½”ë“œë¥¼ ì‹¤í–‰í•˜ëŠ” ë„êµ¬ ì •ì˜
python_repl = PythonREPL()

@st.cache_resource
def get_pdf_retriever(file):
    # ì„ì‹œ íŒŒì¼ ì €ì¥ ë° ë¡œë“œ
    file_path = f"./temp_{file.name}"
    with open(file_path, "wb") as f:
        f.write(file.read())
    
    loader = PyPDFLoader(file_path)
    docs = loader.load()
    
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    splits = text_splitter.split_documents(docs)
    
    vectorstore = FAISS.from_documents(documents=splits, embedding=OpenAIEmbeddings())
    retriever = vectorstore.as_retriever()
    return retriever

# Python ì½”ë“œë¥¼ ì‹¤í–‰í•˜ëŠ” ë„êµ¬ ì •ì˜
@tool
def python_repl_tool(
    code: Annotated[str, "The python code to execute to generate your chart."],
):
    """Use this to execute python code. If you want to see the output of a value,
    you should print it out with `print(...)`. This is visible to the user."""
    try:
        # ì£¼ì–´ì§„ ì½”ë“œë¥¼ Python REPLì—ì„œ ì‹¤í–‰í•˜ê³  ê²°ê³¼ ë°˜í™˜
        result = python_repl.run(code)
    except BaseException as e:
        return f"Failed to execute code. Error: {repr(e)}"
    # ì‹¤í–‰ ì„±ê³µ ì‹œ ê²°ê³¼ì™€ í•¨ê»˜ ì„±ê³µ ë©”ì‹œì§€ ë°˜í™˜
    result_str = f"Successfully executed:\n```python\n{code}\n```\nStdout: {result}"
    return (
        result_str + "\n\nIf you have completed all tasks, respond with FINAL ANSWER."
    )

def make_system_prompt(suffix: str) -> str:
    return (
        "You are a helpful AI assistant, collaborating with other assistants."
        " Use the provided tools to progress towards answering the question."
        " If you are unable to fully answer, that's OK, another assistant with different tools "
        " will help where you left off. Execute what you can to make progress."
        " If you or any of the other assistants have the final answer or deliverable,"
        " prefix your response with FINAL ANSWER so the team knows to stop."
        f"\n{suffix}"
    )

# --- 3. ë…¸ë“œ í•¨ìˆ˜ ì •ì˜ ---

def retrieve(state: AgentState):
    print("\n==== RETRIEVE ====\n")
    query = state["messages"][-1].content
    # st.session_stateì— ì €ì¥ëœ retriever ì‚¬ìš©
    if "pdf_retriever" not in st.session_state:
        return {"documents": []}
        
    docs = st.session_state["pdf_retriever"].invoke(query)
    return {"documents": docs}

def grade_documents(state: AgentState):
    print("\n==== [CHECK DOCUMENT RELEVANCE TO QUESTION] ====\n")
    question = state["messages"][-1].content
    documents = state["documents"]
    
    llm = st.session_state["llm_model"]
    structured_llm_grader = llm.with_structured_output(GradeDocuments)

    # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì •ì˜
    system = """You are a grader assessing relevance of a retrieved document to a user question. \n 
    If the document contains keyword(s) or semantic meaning related to the question, grade it as relevant. \n
    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question."""

    grade_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", system),
            ("human", "Retrieved document: \n\n {document} \n\n User question: {question}") 
        ]
    )

    retrieval_grader = grade_prompt | structured_llm_grader

    filter_docs = []
    relevant_docs = 0

    for doc in documents:
        score = retrieval_grader.invoke(
            {"question": question, "document": doc.page_content}
        )
        grade = score.binary_score

        if grade == "yes":
            print("==== [GRADE: DOCUMENT RELEVANT] ====")
            filter_docs.append(doc)
            relevant_docs += 1
        else:
            print("==== [GRADE: DOCUMENT NOT RELEVANT] ====")
            continue
    return {"documents": filter_docs}

# Research Agent ë…¸ë“œ ì •ì˜
def research_node(state: AgentState) -> AgentState:
    
    llm = st.session_state["llm_model"]

    # Research Agent ìƒì„±
    research_agent = create_react_agent(
        llm,
        tools=[tavily_tool],
        prompt=make_system_prompt(
            "You can only do research. You are working with a chart generator colleague."
        ),
    )

    result = research_agent.invoke(state)

    # ë§ˆì§€ë§‰ ë©”ì‹œì§€ë¥¼ HumanMessage ë¡œ ë³€í™˜
    last_message = HumanMessage(
        content=result["messages"][-1].content, name="researcher"
    )
    return {
        # Research Agent ì˜ ë©”ì‹œì§€ ëª©ë¡ ë°˜í™˜
        "messages": [last_message],
    }

def chart_node(state: AgentState) -> AgentState:

    llm = st.session_state["llm_model"]

    documents = state.get("documents", [])
    context_str = "\n\n".join([doc.page_content for doc in documents])
    chart_generator_system_prompt = f"""
    You can only generate charts. You are working with a researcher colleague.
    Be sure to use the following font code in your code when generating charts.

    IMPORTANT: 
    1. Do NOT use `plt.show()`. It will not work.
    2. Instead, save the chart as an image file named 'chart_output.png' using `plt.savefig('chart_output.png')`.
    3. After saving, print "Chart saved to chart_output.png".

    Here is the data you retrieved from the PDF:
    -----
    {context_str}
    -----
    """
    
    # Chart Generator Agent ìƒì„±
    chart_agent = create_react_agent(
        llm,
        [python_repl_tool],
        prompt=make_system_prompt(chart_generator_system_prompt),
    )

    result = chart_agent.invoke(state)

    # ë§ˆì§€ë§‰ ë©”ì‹œì§€ë¥¼ HumanMessage ë¡œ ë³€í™˜
    last_message = HumanMessage(
        content=result["messages"][-1].content, name="chart_generator"
    )
    return {
        # share internal message history of chart agent with other agents
        "messages": [last_message],
    }

def decide_to_generation(state: AgentState):
    # í‰ê°€ëœ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹¤ìŒ ë‹¨ê³„ ê²°ì •
    print("==== [ASSESS GRADED DOCUMENTS] ====")
    filtered_documents = state["documents"]

    if not filtered_documents:
        # ì›¹ ê²€ìƒ‰ìœ¼ë¡œ ì •ë³´ ë³´ê°•ì´ í•„ìš”í•œ ê²½ìš°
        print(
            "==== [DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, RESEARCHER] ===="
        )
        # ì¿¼ë¦¬ ì¬ì‘ì„± ë…¸ë“œë¡œ ë¼ìš°íŒ…
        return "researcher"
    else:
        # ê´€ë ¨ ë¬¸ì„œê°€ ì¡´ì¬í•˜ë¯€ë¡œ ë‹µë³€ ìƒì„± ë‹¨ê³„(generate) ë¡œ ì§„í–‰
        print("==== [DECISION: GENERATE] ====")
        return "chart_generator"
    
def router(state: AgentState):
    # This is the router
    messages = state["messages"]
    last_message = messages[-1]
    if "FINAL ANSWER" in last_message.content:
        # Any agent decided the work is done
        return END
    return "continue"

# --- 4. ê·¸ë˜í”„ ë¹Œë“œ í•¨ìˆ˜ ---

def build_graph():
    workflow = StateGraph(AgentState)
    workflow.add_node("retrieve", retrieve)
    workflow.add_node("grade_documents", grade_documents)
    workflow.add_node("researcher", research_node)
    workflow.add_node("chart_generator", chart_node)

    workflow.add_edge("retrieve", "grade_documents")
    workflow.add_conditional_edges(
        "grade_documents",
        decide_to_generation,
        {"researcher": "researcher", "chart_generator": "chart_generator"},
    )

    workflow.add_conditional_edges(
        "researcher",
        router,
        {"continue": "chart_generator", END: END},
    )
    workflow.add_conditional_edges(
        "chart_generator",
        router,
        {"continue": "researcher", END: END},
    )

    workflow.add_edge(START, "retrieve")

    return workflow.compile(checkpointer=MemorySaver())

# --- 5. Streamlit ì‚¬ì´ë“œë°” ë° ì´ˆê¸°í™” ---

with st.sidebar:
    st.header("ì„¤ì •")
    uploaded_file = st.file_uploader("PDF íŒŒì¼ì„ ì—…ë¡œë“œ (ì„ íƒ)", type=["pdf"])
    
    model_name = st.selectbox(
        "OpenAI ëª¨ë¸ ì„ íƒ",
        ["gpt-4o-mini", "gpt-4-turbo", "gpt-3.5-turbo"],
        index=0
    )
    
    if st.button("ëŒ€í™” ì´ˆê¸°í™”"):
        st.session_state["messages"] = []
        st.rerun()

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state["messages"] = []

# ëª¨ë¸ ë° ë„êµ¬ ì„¤ì • (ë§¤ë²ˆ ê°±ì‹ )
st.session_state["llm_model"] = ChatOpenAI(model=model_name, temperature=0)

# ê¸°ë³¸ ë„êµ¬: ì›¹ ê²€ìƒ‰
tools = [TavilySearchResults(max_results=3)]

# PDFê°€ ìˆìœ¼ë©´ ë„êµ¬ì— ì¶”ê°€
if uploaded_file:
    retriever = get_pdf_retriever(uploaded_file)
    st.session_state["pdf_retriever"] = retriever
    retriever_tool = create_retriever_tool(
        retriever,
        "pdf_search",
        "Search for information about the uploaded PDF file."
    )
    tools.append(retriever_tool)
    st.success("PDF ì²˜ë¦¬ ì™„ë£Œ! ë„êµ¬ì— ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤.")

st.session_state["tools"] = tools

# ê·¸ë˜í”„ ìƒì„± (í•œ ë²ˆë§Œ í˜¹ì€ íŒŒì¼ ë³€ê²½ ì‹œ)
if "graph" not in st.session_state or uploaded_file:
    st.session_state["graph"] = build_graph()

# --- 6. ì±„íŒ… ì¸í„°í˜ì´ìŠ¤ ---

# ì´ì „ ë©”ì‹œì§€ ì¶œë ¥
for msg in st.session_state["messages"]:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

# ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
if prompt := st.chat_input("ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?"):
    # ì‚¬ìš©ì ë©”ì‹œì§€ ì €ì¥ ë° ì¶œë ¥
    st.session_state["messages"].append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # ê·¸ë˜í”„ ì‹¤í–‰ ì„¤ì •
    config = {"configurable": {"thread_id": "1"}}
    
    # ìŠ¤íŠ¸ë¦¬ë° ì‹¤í–‰ (ì…ë ¥ í‚¤ëŠ” 'input' ì´ì–´ì•¼ í•¨)
    inputs = {"messages": [HumanMessage(content=prompt)]}
    
    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        full_response = ""
        
        # ì´ë²¤íŠ¸ ë£¨í”„: ì‹¤ì œ ê·¸ë˜í”„ì˜ ë…¸ë“œ ì´ë¦„ì— ë§ì¶° ì²˜ë¦¬
        events = st.session_state["graph"].stream(inputs, config=config)
        
        for event in events:
            for node_name, values in event.items():
                # ë©”ì‹œì§€ ëª©ë¡ì—ì„œ ê°€ì¥ ìµœì‹  ë©”ì‹œì§€ ê°€ì ¸ì˜¤ê¸°
                if "messages" in values:
                    latest_message = values["messages"][-1]
                    content = latest_message.content
                else:
                    continue # ë©”ì‹œì§€ê°€ ì—†ëŠ” ê²½ìš° ìŠ¤í‚µ
                
                # 1. Researcher ì—ì´ì „íŠ¸ì˜ ì‘ë‹µ ì²˜ë¦¬
                if node_name == "researcher":
                    with st.expander("ğŸ•µï¸ Researcher (ìë£Œ ì¡°ì‚¬ ì¤‘...)", expanded=False):
                        st.markdown(content)
                    # ì§„í–‰ ìƒí™©ì„ ì ì‹œ ë³´ì—¬ì£¼ê¸° ìœ„í•´ full_response ì—…ë°ì´íŠ¸ (ì„ íƒ ì‚¬í•­)
                    full_response = content 
                        
                # 2. Chart Generator ì—ì´ì „íŠ¸ì˜ ì‘ë‹µ ì²˜ë¦¬
                elif node_name == "chart_generator":
                    with st.expander("ğŸ“Š Chart Generator (ì°¨íŠ¸ ìƒì„± ì¤‘...)", expanded=True):
                        st.markdown(content)
                        
                        # [ì¶”ê°€] ì°¨íŠ¸ ì´ë¯¸ì§€ê°€ ìƒì„±ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ê³  ì¶œë ¥
                        chart_file = "chart_output.png"
                        if os.path.exists(chart_file):
                            # ìºì‹œ ë¬¸ì œ ë°©ì§€ë¥¼ ìœ„í•´ ì´ë¯¸ì§€ë¥¼ ì—´ì–´ì„œ ë°”ë¡œ í‘œì‹œ
                            st.image(chart_file, caption="Generated Chart")
                            
                    full_response = content

                # 3. ê¸°íƒ€ ë…¸ë“œ í˜¹ì€ ì¢…ë£Œ ì¡°ê±´ ì²˜ë¦¬ (í•„ìš”ì‹œ ì¶”ê°€)
                
                # "FINAL ANSWER"ê°€ í¬í•¨ë˜ì–´ ìˆìœ¼ë©´ ìµœì¢… ë‹µë³€ìœ¼ë¡œ ê°„ì£¼í•˜ê³  ì •ì œ
                if "FINAL ANSWER" in content:
                    full_response = content.replace("FINAL ANSWER", "").strip()

                # ì‹¤ì‹œê°„ìœ¼ë¡œ ë©”ì¸ ë©”ì‹œì§€ ì—…ë°ì´íŠ¸ (í˜„ì¬ ë‹¨ê³„ì˜ ê²°ê³¼ë¬¼ì„ ê³„ì† ë³´ì—¬ì¤Œ)
                message_placeholder.markdown(full_response)

    # ìµœì¢… ì‘ë‹µ ì €ì¥
    st.session_state["messages"].append({"role": "assistant", "content": full_response})