import streamlit as st
import operator
import os
from typing import List, Union, Tuple, Annotated, TypedDict, Optional, Sequence, Literal
from dotenv import load_dotenv
from langchain_teddynote import logging
from enum import Enum
import matplotlib.pyplot as plt

# LangChain & LangGraph Imports
from langchain_openai import ChatOpenAI
from langchain_community.tools.tavily_search import TavilySearchResults
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.messages import BaseMessage, HumanMessage
from langchain_core.documents import Document
from pydantic import BaseModel, Field
from langchain_core.output_parsers import StrOutputParser
from langgraph.graph import END, StateGraph, START, MessagesState
from langgraph.checkpoint.memory import MemorySaver
from langgraph.prebuilt import create_react_agent
from langchain_core.tools import create_retriever_tool
from langchain_community.document_loaders import PyPDFLoader
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter

from langchain_teddynote.tools.tavily import TavilySearch
from langchain_core.tools import tool
from langchain_experimental.utilities import PythonREPL
import functools

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ (.env íŒŒì¼ì— OPENAI_API_KEY, TAVILY_API_KEY í•„ìš”)
load_dotenv()

logging.langsmith("Hierarchial Agent")

st.set_page_config(page_title="Hierarchial Agent", layout="wide")
st.title("ğŸ¤– Hierarchial Agent")

# ë©¤ë²„ Agent ëª©ë¡ ì •ì˜
members = ["retrieve", "Researcher", "Coder"]
# ë‹¤ìŒ ì‘ì—…ì ì„ íƒ ì˜µì…˜ ëª©ë¡ ì •ì˜
options_for_next = ["FINISH"] + members


# --- 1. ìƒíƒœ ë° ëª¨ë¸ ì •ì˜ ---
class MessageType(Enum):
    TEXT = "text"
    FIGURE = "figure"
    CODE = "code"


# ìƒíƒœ ì •ì˜
class AgentState(TypedDict):
    messages: Annotated[
        Sequence[BaseMessage], operator.add
    ]  # Agent ê°„ ê³µìœ í•˜ëŠ” ë©”ì‹œì§€ ëª©ë¡
    next: str  # ë‹¤ìŒìœ¼ë¡œ ë¼ìš°íŒ…í•  ì—ì´ì „íŠ¸
    documents: Annotated[List[Document], "The documents retrieved"]


class GradeDocuments(BaseModel):
    """A binary score to determine the relevance of the retrieved documents."""

    # ë¬¸ì„œê°€ ì§ˆë¬¸ì— ê´€ë ¨ì´ ìˆëŠ”ì§€ ì—¬ë¶€ë¥¼ 'yes' ë˜ëŠ” 'no'ë¡œ ë‚˜íƒ€ë‚´ëŠ” í•„ë“œ
    binary_score: str = Field(
        description="Documents are relevant to the question, 'yes' or 'no'"
    )


class RouteResponse(BaseModel):

    next: Literal["FINISH", "retrieve", "Researcher", "Coder"]


# Tavily ê²€ìƒ‰ ë„êµ¬ ì •ì˜
tavily_tool = TavilySearch(max_results=5)

# Python ì½”ë“œë¥¼ ì‹¤í–‰í•˜ëŠ” ë„êµ¬ ì •ì˜
python_repl = PythonREPL()


@st.cache_resource
def get_pdf_retriever(file):
    # ì„ì‹œ íŒŒì¼ ì €ì¥ ë° ë¡œë“œ
    file_path = f"./temp_{file.name}"
    with open(file_path, "wb") as f:
        f.write(file.read())

    loader = PyPDFLoader(file_path)
    docs = loader.load()

    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    splits = text_splitter.split_documents(docs)

    vectorstore = FAISS.from_documents(documents=splits, embedding=OpenAIEmbeddings())
    retriever = vectorstore.as_retriever()
    return retriever


# Python ì½”ë“œë¥¼ ì‹¤í–‰í•˜ëŠ” ë„êµ¬ ì •ì˜
@tool
def python_repl_tool(
    code: Annotated[str, "The python code to execute to generate your chart."],
):
    """Use this to execute python code."""
    try:
        result = python_repl.run(code)
    except BaseException as e:
        return f"Failed to execute code. Error: {repr(e)}"

    result_str = f"Successfully executed:\n```python\n{code}\n```\nStdout: {result}"

    # 1. ê·¸ë¦¼ì´ ê°ì§€ë˜ë©´ íŒŒì¼ë¡œ ì €ì¥ (ì•ˆì „í•œ ì „ë‹¬ì„ ìœ„í•´)
    if plt.get_fignums():
        chart_file = "chart_output.png"
        try:
            plt.savefig(chart_file)
            plt.close()
            # 2. â˜… ì•½ì†í–ˆë˜ "íŠ¹ìˆ˜ íƒœê·¸" ë¶™ì´ê¸°!
            result_str += "\n\n[FIGURE_GENERATED]"
        except Exception as e:
            result_str += f"\n(Chart save failed: {e})"

    return (
        result_str + "\n\nIf you have completed all tasks, respond with FINAL ANSWER."
    )


# ì§€ì •í•œ agentì™€ nameì„ ì‚¬ìš©í•˜ì—¬ agent ë…¸ë“œë¥¼ ìƒì„±
def agent_node(state, agent, name):
    # agent í˜¸ì¶œ
    agent_response = agent.invoke(state)
    # agentì˜ ë§ˆì§€ë§‰ ë©”ì‹œì§€ë¥¼ HumanMessageë¡œ ë³€í™˜í•˜ì—¬ ë°˜í™˜
    return {
        "messages": [
            HumanMessage(content=agent_response["messages"][-1].content, name=name)
        ]
    }


def make_system_prompt(suffix: str) -> str:
    return (
        "You are a helpful AI assistant, collaborating with other assistants."
        " Use the provided tools to progress towards answering the question."
        " If you are unable to fully answer, that's OK, another assistant with different tools "
        " will help where you left off. Execute what you can to make progress."
        " If you or any of the other assistants have the final answer or deliverable,"
        " prefix your response with FINAL ANSWER so the team knows to stop."
        f"\n{suffix}"
    )


# --- 3. ë…¸ë“œ í•¨ìˆ˜ ì •ì˜ ---


def retrieve(state: AgentState):
    print("\n==== RETRIEVE ====\n")
    query = state["messages"][-1].content
    # st.session_stateì— ì €ì¥ëœ retriever ì‚¬ìš©
    if "pdf_retriever" not in st.session_state:
        return {"documents": []}

    docs = st.session_state["pdf_retriever"].invoke(query)
    return {"documents": docs}


def grade_documents(state: AgentState):
    """ê²€ìƒ‰ëœ ë¬¸ì„œ í‰ê°€ ë…¸ë“œ"""
    print("\n==== GRADE ====\n")
    question = state["messages"][-1].content
    documents = state["documents"]

    # 1. ë¬¸ì„œê°€ ì•„ì˜ˆ ì—†ëŠ” ê²½ìš° (ê²€ìƒ‰ ì‹¤íŒ¨)
    if not documents:
        msg = (
            "PDF ë¬¸ì„œì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. "
            "ì¦‰ì‹œ 'Researcher' ì—ì´ì „íŠ¸ë¥¼ í˜¸ì¶œí•˜ì—¬ ì›¹ ê²€ìƒ‰ì„ ìˆ˜í–‰í•˜ì„¸ìš”."
        )
        # Supervisorì—ê²Œ ê°•ë ¥í•œ íŒíŠ¸(ì§€ì‹œ)ë¥¼ ë³´ëƒ„
        return {
            "documents": [],
            "messages": [HumanMessage(content=msg, name="grade_documents")],
        }

    # 2. ë¬¸ì„œê°€ ìˆëŠ” ê²½ìš° í‰ê°€ ì§„í–‰
    llm = st.session_state["llm_model"]
    structured_llm_grader = llm.with_structured_output(GradeDocuments)

    system = """You are a grader assessing relevance of a retrieved document to a user question.
    If the document contains keyword(s) or semantic meaning related to the question, grade it as relevant.
    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question."""

    grade_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", system),
            (
                "human",
                "Retrieved document: \n\n {document} \n\n User question: {question}",
            ),
        ]
    )
    retrieval_grader = grade_prompt | structured_llm_grader

    filter_docs = []
    for doc in documents:
        score = retrieval_grader.invoke(
            {"question": question, "document": doc.page_content}
        )
        if score.binary_score == "yes":
            filter_docs.append(doc)

    # 3. í‰ê°€ í›„ ê´€ë ¨ ë¬¸ì„œê°€ í•˜ë‚˜ë„ ì—†ëŠ” ê²½ìš°
    if not filter_docs:
        msg_content = (
            "ê²€ìƒ‰ëœ ë¬¸ì„œë“¤ì„ ê²€í† í–ˆìœ¼ë‚˜ ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤. "
            "ë”°ë¼ì„œ 'Researcher' ì—ì´ì „íŠ¸ë¥¼ í˜¸ì¶œí•˜ì—¬ ì™¸ë¶€ ì›¹ ê²€ìƒ‰ì„ ìˆ˜í–‰í•´ì•¼ í•©ë‹ˆë‹¤."
        )
    else:
        msg_content = f"PDF ê²€ìƒ‰ ê²°ê³¼ {len(documents)}ê±´ ì¤‘ {len(filter_docs)}ê±´ì˜ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤. ì´ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•˜ê±°ë‚˜, í•„ìš”í•˜ë‹¤ë©´ Coderì—ê²Œ ì‹œê°í™”ë¥¼ ìš”ì²­í•˜ì„¸ìš”."

    return {
        "documents": filter_docs,
        "messages": [HumanMessage(content=msg_content, name="grade_documents")],
    }


# Supervisor Agent ìƒì„±
def supervisor_agent(state):
    system_prompt = (
        "You are a supervisor managing {members}.\n"
        "Your goal is to answer the user's request by routing to the right worker.\n\n"
        "### RULES ###\n"
        "1. **Data First**: If the user wants a chart but data is missing, ALWAYS call 'Researcher' first.\n"
        "2. **No Infinite Loops**: If 'Researcher' just reported an error or failure, DO NOT call them again. Respond with FINISH.\n"
        "3. **Sequence**: 'Researcher' (find data) -> 'Coder' (draw chart) -> FINISH.\n"
        "4. **Completion**: Only respond with FINISH when the chart is displayed or the task is impossible."
    )

    prompt = ChatPromptTemplate.from_messages(
        [
            ("system", system_prompt),
            MessagesPlaceholder(variable_name="messages"),
            (
                "system",
                "Given the conversation above, who should act next? "
                "Select one of: {options}",
            ),
        ]
    ).partial(options=str(options_for_next), members=", ".join(members))

    llm = st.session_state["llm_model"]
    supervisor_chain = prompt | llm.with_structured_output(RouteResponse)
    return supervisor_chain.invoke(state)


# Research Agent ë…¸ë“œ ì •ì˜
def research_node(state: AgentState) -> AgentState:
    # Research Agent ìƒì„±
    research_agent = create_react_agent(ChatOpenAI(model="gpt-4o"), tools=[tavily_tool])

    # research node ìƒì„±
    research_node = functools.partial(
        agent_node, agent=research_agent, name="Researcher"
    )
    return research_node(state)


# Coder Agent ë…¸ë“œ ì •ì˜
def coder_node(state: AgentState) -> AgentState:
    code_system_prompt = """
    You are a python coding assistant.
    
    ### CRITICAL RULES FOR PLOTTING ###
    1. **NO plt.show()**: NEVER use `plt.show()`. It will cause a backend error and delete your plot from memory.
    2. **Just Plot**: Just write the code to create the plot (e.g., `plt.plot(...)`, `plt.title(...)`).
    3. **Auto-Save**: The system will automatically detect the active figure and save it as a file. You do NOT need to save it yourself.
    
    ### KOREAN FONT SETTINGS (Must Include) ###
    import platform
    import matplotlib.pyplot as plt
    import matplotlib.font_manager as fm

    current_os = platform.system()
    if current_os == "Windows":
        font_path = "C:/Windows/Fonts/malgun.ttf"
        fontprop = fm.FontProperties(fname=font_path, size=12)
        plt.rc("font", family=fontprop.get_name())
    elif current_os == "Darwin": # macOS
        plt.rcParams["font.family"] = "AppleGothic"
    else: # Linux
        try:
            plt.rcParams["font.family"] = "NanumGothic"
        except:
            pass
    plt.rcParams["axes.unicode_minus"] = False
    ###########################################
    """

    coder_agent = create_react_agent(
        ChatOpenAI(model="gpt-4o"),
        tools=[python_repl_tool],
        prompt=code_system_prompt,
    )

    coder_node_func = functools.partial(agent_node, agent=coder_agent, name="Coder")
    return coder_node_func(state)


def router(state: AgentState):
    # This is the router
    messages = state["messages"]
    last_message = messages[-1]
    if "FINAL ANSWER" in last_message.content:
        # Any agent decided the work is done
        return END
    return "continue"


# --- 4. ê·¸ë˜í”„ ë¹Œë“œ í•¨ìˆ˜ ---


def build_graph():  # ê·¸ë˜í”„ ìƒì„±
    workflow = StateGraph(AgentState)

    # ê·¸ë˜í”„ì— ë…¸ë“œ ì¶”ê°€
    workflow.add_node("retrieve", retrieve)
    workflow.add_node("grade_documents", grade_documents)
    workflow.add_node("Researcher", research_node)
    workflow.add_node("Coder", coder_node)
    workflow.add_node("Supervisor", supervisor_agent)

    # 1. ì¼ë°˜ ë©¤ë²„ë“¤ (Researcher, Coder)ì€ ì‘ì—… í›„ ë°”ë¡œ Supervisorë¡œ ë³µê·€
    # members ë¦¬ìŠ¤íŠ¸ì—ì„œ "retrieve"ëŠ” ì œì™¸í•˜ê³  ì²˜ë¦¬í•˜ê±°ë‚˜, ì§ì ‘ ì§€ì •í•˜ëŠ” ê²Œ ì•ˆì „í•©ë‹ˆë‹¤.
    workflow.add_edge("Researcher", "Supervisor")
    workflow.add_edge("Coder", "Supervisor")

    # Supervisor -> retrieve (ì¡°ê±´ë¶€ ì—£ì§€ë¡œ ì˜´)
    # retrieve -> grade_documents (ë¬´ì¡°ê±´ ì´ë™)
    workflow.add_edge("retrieve", "grade_documents")

    # grade_documents -> Supervisor (í‰ê°€ ëë‚˜ë©´ ë³µê·€)
    workflow.add_edge("grade_documents", "Supervisor")

    # 3. ì¡°ê±´ë¶€ ì—£ì§€ ì„¤ì •
    # Supervisorê°€ ì„ íƒí•  ìˆ˜ ìˆëŠ” ì˜µì…˜ë“¤ ë§¤í•‘
    conditional_map = {
        "Researcher": "Researcher",
        "Coder": "Coder",
        "retrieve": "retrieve",  # SupervisorëŠ” 'retrieve'ë¥¼ ì„ íƒí•˜ì§€ë§Œ
        "FINISH": END,
    }

    def get_next(state):
        return state["next"]

    workflow.add_conditional_edges("Supervisor", get_next, conditional_map)

    # ì‹œì‘ì 
    workflow.add_edge(START, "Supervisor")

    return workflow.compile(checkpointer=MemorySaver())


# --- 5. Streamlit ì‚¬ì´ë“œë°” ë° ì´ˆê¸°í™” ---

with st.sidebar:
    st.header("ì„¤ì •")
    uploaded_file = st.file_uploader("PDF íŒŒì¼ì„ ì—…ë¡œë“œ (ì„ íƒ)", type=["pdf"])

    model_name = st.selectbox(
        "OpenAI ëª¨ë¸ ì„ íƒ", ["gpt-4o-mini", "gpt-4-turbo", "gpt-3.5-turbo"], index=0
    )

    if st.button("ëŒ€í™” ì´ˆê¸°í™”"):
        st.session_state["messages"] = []
        st.rerun()

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state["messages"] = []

# ëª¨ë¸ ë° ë„êµ¬ ì„¤ì • (ë§¤ë²ˆ ê°±ì‹ )
st.session_state["llm_model"] = ChatOpenAI(model=model_name, temperature=0)

# ê¸°ë³¸ ë„êµ¬: ì›¹ ê²€ìƒ‰
tools = [TavilySearchResults(max_results=3)]

# PDFê°€ ìˆìœ¼ë©´ ë„êµ¬ì— ì¶”ê°€
if uploaded_file:
    retriever = get_pdf_retriever(uploaded_file)
    st.session_state["pdf_retriever"] = retriever
    retriever_tool = create_retriever_tool(
        retriever, "pdf_search", "Search for information about the uploaded PDF file."
    )
    tools.append(retriever_tool)
    st.success("PDF ì²˜ë¦¬ ì™„ë£Œ! ë„êµ¬ì— ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤.")

st.session_state["tools"] = tools

# ê·¸ë˜í”„ ìƒì„± (í•œ ë²ˆë§Œ í˜¹ì€ íŒŒì¼ ë³€ê²½ ì‹œ)
if "graph" not in st.session_state or uploaded_file:
    st.session_state["graph"] = build_graph()

# --- 6. ì±„íŒ… ì¸í„°í˜ì´ìŠ¤ ---

# ì´ì „ ë©”ì‹œì§€ ì¶œë ¥
for msg in st.session_state["messages"]:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

# ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
if prompt := st.chat_input("ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?"):
    # ì‚¬ìš©ì ë©”ì‹œì§€ ì €ì¥ ë° ì¶œë ¥
    st.session_state["messages"].append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # ê·¸ë˜í”„ ì‹¤í–‰ ì„¤ì •
    config = {"configurable": {"thread_id": "1"}}

    # ìŠ¤íŠ¸ë¦¬ë° ì‹¤í–‰ (ì…ë ¥ í‚¤ëŠ” 'input' ì´ì–´ì•¼ í•¨)
    inputs = {"messages": [HumanMessage(content=prompt)]}

    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        full_response = ""

        events = st.session_state["graph"].stream(
            inputs,
            config={
                "configurable": {"thread_id": "1"},
                "recursion_limit": 20,  # ì—¬ê¸°ì„œ ì œí•œì„ ì„¤ì •í•©ë‹ˆë‹¤ (ê¸°ë³¸ê°’ 25)
            },
        )

        for event in events:
            for node_name, values in event.items():
                if "messages" not in values or not values["messages"]:
                    continue

                latest_message = values["messages"][-1]
                content = latest_message.content

                # --- â˜… ì‚¬ìš©ìë‹˜ì´ ì›í•˜ì…¨ë˜ "íƒ€ì… ê²°ì • ë¡œì§" ë³µì› ---
                message_type = MessageType.TEXT

                # Coderê°€ "[FIGURE_GENERATED]" íƒœê·¸ë¥¼ ë‹¬ê³  ì™”ë‹¤ë©´? -> ê·¸ë¦¼ ëª¨ë“œ!
                if node_name == "Coder" and "[FIGURE_GENERATED]" in content:
                    message_type = MessageType.FIGURE
                    # íƒœê·¸ í…ìŠ¤íŠ¸ëŠ” ë³´ê¸° ì‹«ìœ¼ë‹ˆ ì œê±°í•´ì„œ ê¹”ë”í•˜ê²Œ ë§Œë“¦
                    clean_content = content.replace("[FIGURE_GENERATED]", "")
                else:
                    clean_content = content

                # --- â˜… ë Œë”ë§ ë¡œì§ ---
                if node_name == "Coder":
                    with st.expander("ğŸ’» Coder (ì½”ë“œ ì‘ì„± ë° ì‹¤í–‰)", expanded=True):

                        # 1. í…ìŠ¤íŠ¸/ì½”ë“œ ë¨¼ì € ì¶œë ¥
                        st.markdown(clean_content)

                        # 2. ê·¸ë¦¼ íƒ€ì…ì´ë©´ ë Œë”ë§
                        if message_type == MessageType.FIGURE:
                            chart_file = "chart_output.png"
                            if os.path.exists(chart_file):
                                # st.pyplot() ëŒ€ì‹  st.image()ë¥¼ ì“°ì§€ë§Œ,
                                # ë¡œì§ êµ¬ì¡°ëŠ” ì‚¬ìš©ìë‹˜ì˜ ì˜ë„ëŒ€ë¡œ "íƒœê·¸ ê°ì§€ ì‹œ ë Œë”ë§"ì…ë‹ˆë‹¤.
                                st.image(chart_file, caption="Generated Chart")
                            else:
                                st.error("âš ï¸ ê·¸ë˜í”„ íƒœê·¸ëŠ” ìˆëŠ”ë° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")

                    full_response = clean_content

                elif node_name == "Researcher":
                    with st.expander("ğŸ•µï¸ Researcher", expanded=True):
                        st.markdown(content)
                    full_response = content

                elif node_name == "retrieve":
                    with st.expander("ğŸ” Retrieve", expanded=False):
                        st.markdown(content)

                elif node_name == "grade_documents":
                    with st.expander("âš–ï¸ Grade", expanded=False):
                        st.markdown(content)

                # ë©”ì¸ ë©”ì‹œì§€ ì—…ë°ì´íŠ¸ (í…ìŠ¤íŠ¸ë§Œ)
                if "FINAL ANSWER" in full_response:
                    final_view = full_response.replace("FINAL ANSWER", "").strip()
                    message_placeholder.markdown(final_view)
                else:
                    message_placeholder.markdown(full_response)

    # ìŠ¤íŠ¸ë¦¬ë°ì´ ëë‚œ í›„, ìµœì¢… ê²°ê³¼ë¥¼ ì„¸ì…˜ì— ì €ì¥
    st.session_state["messages"].append({"role": "assistant", "content": full_response})
